{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pgeon.policy_graph as PG\n",
    "from pathlib import Path\n",
    "from example.environment import SelfDrivingEnvironment\n",
    "from example.discretizer.discretizer import AVDiscretizer\n",
    "from example.discretizer.discretizer_d1 import AVDiscretizerD1\n",
    "import pandas as pd\n",
    "from example.transition import TransitionRecorded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SelfDrivingEnvironment()\n",
    "disc = AVDiscretizer()\n",
    "    # Generate Policy Graph\n",
    "    #from existing csv file\n",
    "    #pg = PG.PolicyGraph.from_nodes_and_edges(str(Path(data_folder) / 'nuscenes_nodes.csv'), str(Path(data_folder) / 'nuscenes_edges.csv'), env, env.discretizer  )\n",
    "\n",
    "    #from raw data\n",
    "dtype_dict = {\n",
    "        'modality': 'category',  # for limited set of modalities, 'category' is efficient\n",
    "        'scene_token': 'str',  \n",
    "        'steering_angle': 'float64',  \n",
    "        'timestamp': 'str',  # To enable datetime operations\n",
    "        'rotation': 'object',  # Quaternion (lists)\n",
    "        'x': 'float64',\n",
    "        'y': 'float64',\n",
    "        'z': 'float64',\n",
    "        'yaw': 'float64',  \n",
    "        'velocity': 'float64',\n",
    "        'acceleration': 'float64',\n",
    "        #'heading_change_rate': 'float64',\n",
    "        'delta_local_x': 'float64',\n",
    "        'delta_local_y': 'float64'\n",
    "        #'is_destination': 'str'\n",
    "    }\n",
    "\n",
    "df = pd.read_csv(Path('/home/saramontese/Desktop/MasterThesis/example/dataset/data/sets/nuscenes') / \"train_v1.0-trainval_lidar_0.csv\", dtype=dtype_dict, parse_dates=['timestamp'])\n",
    "pg = PG.PolicyGraph(env, disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting PG from scenes...: 100%|██████████| 668/668 [00:39<00:00, 16.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward: 7.928068862275447 and Standard Deviation: 7.697623982975654 --> Epoch Mean Time: 39.65204048156738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pg = pg.fit(df, update=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg._save_csv('/home/saramontese/Desktop/MasterThesis/example/dataset/data/policy_graphs/pg_nodes.csv', '/home/saramontese/Desktop/MasterThesis/example/dataset/data/policy_graphs/pg_edges.csv','/home/saramontese/Desktop/MasterThesis/example/dataset/data/policy_graphs/pg_traj.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode in [PGBasedPolicyMode.GREEDY, PGBasedPolicyMode.STOCHASTIC], \\\n",
    "# node_not_found_mode in [PGBasedPolicyNodeNotFoundMode.RANDOM_UNIFORM,\n",
    "#                                      PGBasedPolicyNodeNotFoundMode.FIND_SIMILAR_NODES], \\\n",
    "   \n",
    "random_agent = PG.PGBasedPolicy(pg, mode=PG.PGBasedPolicyMode.GREEDY, node_not_found_mode=PG.PGBasedPolicyNodeNotFoundMode.RANDOM_UNIFORM)\n",
    "stochastic_agent = PG.PGBasedPolicy(pg, mode=PG.PGBasedPolicyMode.STOCHASTIC, node_not_found_mode=PG.PGBasedPolicyNodeNotFoundMode.FIND_SIMILAR_NODES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "* START TESTING\n",
      "\n",
      "573.6674119753936 2006.1744084989757 7.771227028161312 0.0907571211037099\n",
      "Actual state: (573.6674119753936, 2006.1744084989757, 7.771227028161312, 0.0907571211037099)\n",
      "Action: Action.BRAKE\n",
      "Actual state: (574.0105118958064, 2009.9444419368383, 7.571227028161312, 0.0907571211037099)\n",
      "Action: Action.STRAIGHT\n",
      "Actual state: (574.3536118162192, 2013.714475374701, 7.571227028161312, 0.0907571211037099)\n",
      "Action: Action.STRAIGHT\n",
      "Actual state: (574.696711736632, 2017.4845088125635, 7.571227028161312, 0.0907571211037099)\n",
      "Action: Action.STRAIGHT\n",
      "Actual state: (575.0398116570448, 2021.254542250426, 7.571227028161312, 0.0907571211037099)\n",
      "Action: Action.STRAIGHT\n",
      "No nearest states available.\n",
      "Actual state: (575.3829115774575, 2025.0245756882887, 7.571227028161312, 0.0907571211037099)\n",
      "Action: Action.IDLE\n",
      "No nearest states available.\n",
      "Actual state: (575.7260114978703, 2028.7946091261513, 7.571227028161312, 0.0907571211037099)\n",
      "Action: Action.BRAKE_TURN_RIGHT\n",
      "No nearest states available.\n",
      "Actual state: (574.2594511777347, 2032.1758733575562, 7.371227028161312, -0.4092428788962901)\n",
      "Action: Action.STRAIGHT\n",
      "No nearest states available.\n",
      "Actual state: (572.7928908575991, 2035.557137588961, 7.371227028161312, -0.4092428788962901)\n",
      "Action: Action.IDLE\n",
      "No nearest states available.\n",
      "Actual state: (571.3263305374635, 2038.938401820366, 7.371227028161312, -0.4092428788962901)\n",
      "Action: Action.GAS_TURN_LEFT\n",
      "No nearest states available.\n",
      "Actual state: (569.8199787331005, 2042.4114082873168, 7.571227028161312, -0.4092428788962901)\n",
      "Action: Action.BRAKE\n",
      "No nearest states available.\n",
      "Actual state: (568.3534184129649, 2045.7926725187217, 7.371227028161312, -0.4092428788962901)\n",
      "Action: Action.GAS_TURN_LEFT\n",
      "No nearest states available.\n",
      "Actual state: (566.8470666086018, 2049.2656789856724, 7.571227028161312, -0.4092428788962901)\n",
      "Action: Action.IDLE\n",
      "No nearest states available.\n",
      "Actual state: (565.3407148042388, 2052.738685452623, 7.571227028161312, -0.4092428788962901)\n",
      "Action: Action.IDLE\n",
      "No nearest states available.\n",
      "Actual state: (563.8343629998758, 2056.2116919195737, 7.571227028161312, -0.4092428788962901)\n",
      "Action: Action.TURN_LEFT\n",
      "No nearest states available.\n",
      "Actual state: (562.3280111955128, 2059.6846983865244, 7.571227028161312, -0.4092428788962901)\n",
      "Action: Action.TURN_RIGHT\n",
      "No nearest states available.\n",
      "Actual state: (559.341015123938, 2062.010364774119, 7.571227028161312, -0.9092428788962901)\n",
      "Action: Action.GAS_TURN_LEFT\n",
      "No nearest states available.\n",
      "Actual state: (556.275115169004, 2064.39746549406, 7.771227028161312, -0.9092428788962901)\n",
      "Action: Action.GAS_TURN_RIGHT\n",
      "No nearest states available.\n",
      "Actual state: (552.3413999237181, 2065.0385578841465, 7.971227028161312, -1.40924287889629)\n",
      "Action: Action.STRAIGHT\n",
      "No nearest states available.\n",
      "Actual state: (548.4076846784321, 2065.679650274233, 7.971227028161312, -1.40924287889629)\n",
      "Action: Action.BRAKE_TURN_RIGHT\n",
      "No nearest states available.\n",
      "Actual state: (544.742495869576, 2064.389540443156, 7.771227028161312, -1.90924287889629)\n",
      "Action: Action.TURN_RIGHT\n",
      "No nearest states available.\n",
      "Actual state: (542.1445016855108, 2061.5001774337074, 7.771227028161312, -2.40924287889629)\n",
      "Action: Action.BRAKE_TURN_RIGHT\n",
      "No nearest states available.\n",
      "Actual state: (541.2728081895808, 2057.8162908818244, 7.571227028161312, -2.90924287889629)\n",
      "Action: Action.BRAKE_TURN_LEFT\n",
      "No nearest states available.\n",
      "Actual state: (540.4241411725211, 2054.229717131108, 7.371227028161312, -2.90924287889629)\n",
      "Action: Action.BRAKE\n",
      "No nearest states available.\n",
      "Actual state: (539.5985006343317, 2050.740456181558, 7.1712270281613115, -2.90924287889629)\n",
      "Action: Action.TURN_LEFT\n",
      "No nearest states available.\n",
      "Actual state: (538.7728600961423, 2047.2511952320076, 7.1712270281613115, -2.90924287889629)\n",
      "Action: Action.GAS_TURN_RIGHT\n",
      "No nearest states available.\n",
      "Actual state: (539.7475797733001, 2043.6968080096956, 7.371227028161312, -3.40924287889629)\n",
      "Action: Action.GAS\n",
      "No nearest states available.\n",
      "Actual state: (540.7487460562519, 2040.0459812879967, 7.571227028161312, -3.40924287889629)\n",
      "Action: Action.GAS_TURN_LEFT\n",
      "Actual state: (541.7763589449977, 2036.298715066911, 7.771227028161312, -3.40924287889629)\n",
      "Action: Action.GAS\n",
      "Actual state: (542.8304184395374, 2032.4550093464384, 7.971227028161312, -3.40924287889629)\n",
      "Action: Action.GAS\n",
      "Actual state: (543.9109245398712, 2028.5148641265791, 8.171227028161312, -3.40924287889629)\n",
      "Action: Action.GAS\n",
      "Actual state: (545.0178772459989, 2024.478279407333, 8.37122702816131, -3.40924287889629)\n",
      "Action: Action.GAS\n",
      "Actual state: (546.1512765579205, 2020.3452551887, 8.57122702816131, -3.40924287889629)\n",
      "Action: Action.GAS\n",
      "Actual state: (547.3111224756361, 2016.1157914706803, 8.77122702816131, -3.40924287889629)\n",
      "Action: Action.GAS\n",
      "Actual state: (548.4974149991457, 2011.7898882532738, 8.971227028161309, -3.40924287889629)\n",
      "Action: Action.GAS\n",
      "No nearest states available.\n",
      "Actual state: (549.7101541284492, 2007.3675455364805, 9.171227028161308, -3.40924287889629)\n",
      "Action: Action.BRAKE_TURN_RIGHT\n",
      "Actual state: (552.8251722403369, 2004.1399472404848, 8.971227028161309, -3.90924287889629)\n",
      "Action: Action.GAS\n",
      "No nearest states available.\n",
      "Actual state: (556.0096349912268, 2000.8403944997767, 9.171227028161308, -3.90924287889629)\n",
      "Action: Action.GAS_TURN_LEFT\n",
      "No nearest states available.\n",
      "Actual state: (559.2635423811189, 1997.4688873143564, 9.371227028161307, -3.90924287889629)\n",
      "Action: Action.TURN_RIGHT\n",
      "No nearest states available.\n",
      "Actual state: (563.7355014127725, 1996.0701177041108, 9.371227028161307, -4.40924287889629)\n",
      "Action: Action.GAS_TURN_RIGHT\n",
      "Average Reward: -0.1 and Standard Deviation: 0.1479019945774904 --> Episode Mean Time: 0.15996551513671875\n",
      "* END TESTING\n",
      "---------------------------------\n",
      "* RESULTS\n",
      "---------------------------------\n",
      "* COMPARATIVE\n",
      "\t- Original RL Agent\n",
      "\t\t+ Average Episode Reward: 7.928068862275447\n",
      "\t\t+ Standard Deviation: 7.697623982975654\n",
      "\t- Policy Graph\n",
      "\t\t+ Average Episode Reward: -0.1\n",
      "\t\t+ Standard Deviation: 0.1479019945774904\n",
      "\t- Difference\n",
      "\t\t+ Average Episode Reward Diff: -8.028068862275447\n",
      "\t\t+ Standard Deviation Diff: -7.549721988398163\n",
      "\t- Transferred Learning: -1 %\n"
     ]
    }
   ],
   "source": [
    "stochastic_agent.test(num_episodes=1, data_file=Path('/home/saramontese/Desktop/MasterThesis/example/dataset/data/sets/nuscenes') / \"train_v1.0-trainval_lidar_0.csv\",  seed=42, max_steps=40, verbose = True)\n",
    "diff_aer, diff_std, transferred_learning = stochastic_agent.compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "* START TESTING\n",
      "\n",
      "Actual state: (427, 1622, 1.794, 0.179)\n",
      "Action: Action.GAS\n",
      "Actual state: (427.17751150346106, 1622.9810701637186, 1.994, 0.179)\n",
      "Action: Action.GAS\n",
      "Actual state: (427.3728275709604, 1624.0605425504782, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (427.56814363845973, 1625.1400149372378, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (427.76345970595906, 1626.2194873239973, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (427.9587757734584, 1627.298959710757, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (428.15409184095773, 1628.3784320975165, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (428.34940790845707, 1629.457904484276, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (428.5447239759564, 1630.5373768710356, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (428.74004004345574, 1631.6168492577951, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (428.93535611095507, 1632.6963216445547, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (429.1306721784544, 1633.7757940313143, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (429.32598824595374, 1634.8552664180738, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (429.5213043134531, 1635.9347388048334, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (429.7166203809524, 1637.014211191593, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (429.91193644845174, 1638.0936835783525, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (430.1072525159511, 1639.173155965112, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (430.3025685834504, 1640.2526283518716, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (430.49788465094974, 1641.3321007386312, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (430.6932007184491, 1642.4115731253908, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (430.8885167859484, 1643.4910455121503, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (431.08383285344775, 1644.5705178989099, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (431.2791489209471, 1645.6499902856694, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (431.4744649884464, 1646.729462672429, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (431.66978105594575, 1647.8089350591886, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (431.8650971234451, 1648.8884074459481, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (432.0604131909444, 1649.9678798327077, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (432.25572925844375, 1651.0473522194673, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (432.4510453259431, 1652.1268246062268, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (432.6463613934424, 1653.2062969929864, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (432.84167746094175, 1654.285769379746, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (433.0369935284411, 1655.3652417665055, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (433.2323095959404, 1656.444714153265, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (433.42762566343976, 1657.5241865400246, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (433.6229417309391, 1658.6036589267842, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (433.8182577984384, 1659.6831313135438, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (434.01357386593776, 1660.7626037003033, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (434.2088899334371, 1661.8420760870629, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (434.4042060009364, 1662.9215484738224, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Actual state: (434.59952206843576, 1664.001020860582, 2.194, 0.179)\n",
      "Action: Action.IDLE\n",
      "Average Reward: 0.20000000000000004 and Standard Deviation: 0.11180339887498948 --> Episode Mean Time: 0.005570411682128906\n",
      "* END TESTING\n",
      "---------------------------------\n",
      "* RESULTS\n",
      "---------------------------------\n",
      "* COMPARATIVE\n",
      "\t- Original RL Agent\n",
      "\t\t+ Average Episode Reward: 7.928068862275447\n",
      "\t\t+ Standard Deviation: 7.697623982975654\n",
      "\t- Policy Graph\n",
      "\t\t+ Average Episode Reward: 0.20000000000000004\n",
      "\t\t+ Standard Deviation: 0.11180339887498948\n",
      "\t- Difference\n",
      "\t\t+ Average Episode Reward Diff: -7.728068862275447\n",
      "\t\t+ Standard Deviation Diff: -7.585820584100665\n",
      "\t- Transferred Learning: 2 %\n"
     ]
    }
   ],
   "source": [
    "random_agent.test(num_episodes=1, data_file = Path('/home/saramontese/Desktop/MasterThesis/example/dataset/data/sets/nuscenes') / \"train_v1.0-trainval_lidar_0.csv\", seed=42, max_steps=40, verbose = True)\n",
    "diff_aer, diff_std, transferred_learning = random_agent.compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST POLICY ITERATION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
