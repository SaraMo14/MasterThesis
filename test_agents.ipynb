{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pgeon.policy_graph as PG\n",
    "from pathlib import Path\n",
    "from example.environment import SelfDrivingEnvironment\n",
    "from example.discretizer.discretizer import AVDiscretizer\n",
    "from example.discretizer.discretizer_d1 import AVDiscretizerD1\n",
    "import pandas as pd\n",
    "from example.transition import TransitionRecorded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SelfDrivingEnvironment()\n",
    "disc = AVDiscretizer()\n",
    "    # Generate Policy Graph\n",
    "    #from existing csv file\n",
    "    #pg = PG.PolicyGraph.from_nodes_and_edges(str(Path(data_folder) / 'nuscenes_nodes.csv'), str(Path(data_folder) / 'nuscenes_edges.csv'), env, env.discretizer  )\n",
    "\n",
    "    #from raw data\n",
    "dtype_dict = {\n",
    "        'modality': 'category',  # for limited set of modalities, 'category' is efficient\n",
    "        'scene_token': 'str',  \n",
    "        'steering_angle': 'float64',  \n",
    "        'timestamp': 'str',  # To enable datetime operations\n",
    "        'rotation': 'object',  # Quaternion (lists)\n",
    "        'x': 'float64',\n",
    "        'y': 'float64',\n",
    "        'z': 'float64',\n",
    "        'yaw': 'float64',  \n",
    "        'velocity': 'float64',\n",
    "        'acceleration': 'float64',\n",
    "        #'heading_change_rate': 'float64',\n",
    "        'delta_local_x': 'float64',\n",
    "        'delta_local_y': 'float64'\n",
    "        #'is_destination': 'str'\n",
    "    }\n",
    "\n",
    "df = pd.read_csv(Path('/home/saramontese/Desktop/MasterThesis/example/dataset/data/sets/nuscenes') / \"train_v1.0-mini_lidar_0.csv\", dtype=dtype_dict, parse_dates=['timestamp'])\n",
    "pg = PG.PolicyGraph(env, disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting PG from scenes...: 100%|██████████| 8/8 [00:00<00:00, 17.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward: 2.887500000000001 and Standard Deviation: 4.6412383853881085 --> Epoch Mean Time: 0.45798540115356445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AER': [2.887500000000001], 'STD': [4.6412383853881085]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pg = pg.fit(df, update=False, verbose=False)\n",
    "pg.agent_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mode in [PGBasedPolicyMode.GREEDY, PGBasedPolicyMode.STOCHASTIC], \\\n",
    "# node_not_found_mode in [PGBasedPolicyNodeNotFoundMode.RANDOM_UNIFORM,\n",
    "#                                      PGBasedPolicyNodeNotFoundMode.FIND_SIMILAR_NODES], \\\n",
    "   \n",
    "random_agent = PG.PGBasedPolicy(pg, mode=PG.PGBasedPolicyMode.GREEDY, node_not_found_mode=PG.PGBasedPolicyNodeNotFoundMode.RANDOM_UNIFORM)\n",
    "stochastic_agent = PG.PGBasedPolicy(pg, mode=PG.PGBasedPolicyMode.STOCHASTIC, node_not_found_mode=PG.PGBasedPolicyNodeNotFoundMode.FIND_SIMILAR_NODES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "* START TESTING\n",
      "\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Actual state: (329.6474941596216, 660.1966888688361, 5.108549775006556, -0.20238621771937623)\n",
      "Action: Action.IDLE\n",
      "Average Reward: 10.0 and Standard Deviation: 0.0 --> Episode Mean Time: 0.00203704833984375\n",
      "* END TESTING\n",
      "---------------------------------\n",
      "* RESULTS\n",
      "---------------------------------\n",
      "* COMPARATIVE\n",
      "\t- Original RL Agent\n",
      "\t\t+ Average Episode Reward: 2.887500000000001\n",
      "\t\t+ Standard Deviation: 4.6412383853881085\n",
      "\t- Policy Graph\n",
      "\t\t+ Average Episode Reward: 10.0\n",
      "\t\t+ Standard Deviation: 0.0\n",
      "\t- Difference\n",
      "\t\t+ Average Episode Reward Diff: 7.112499999999999\n",
      "\t\t+ Standard Deviation Diff: -4.6412383853881085\n",
      "\t- Transferred Learning: 346 %\n"
     ]
    }
   ],
   "source": [
    "random_agent.test(num_episodes=1, seed=42, max_steps=20, verbose = True)\n",
    "diff_aer, diff_std, transferred_learning = random_agent.compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST POLICY ITERATION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
